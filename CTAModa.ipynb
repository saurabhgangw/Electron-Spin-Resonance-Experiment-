{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHdM5yr9rJoCd97LJ2rGrG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saurabhgangw/Electron-Spin-Resonance-Experiment-/blob/master/CTAModa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "C7-Gj-xJ2TM1",
        "outputId": "963149e4-aa65-47b5-f2b4-be1c4f220a55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['203195171', '2526222', '39126115', '9227178', '14913244', '20413093', '25524679', '40204289', '94285256', '150133253', '20558117', '256201269', '416282', '96173210', '15171159', '206132181', '257147137', '42137170', '9716334', '1526190', '20715796', '258216262', '4279242', '974126', '1537527', '2085035', '2594722', '43229198', '9833127', '154213123', '20935163', '26016181', '44121277', '99196136', '1545216', '21025892', '261227119', '45281252']\n",
            "['24590248', '323173', '8312234', '14119219', '19425536', '246120151', '323440', '851976', '14240153', '1952918', '247289268', '33139299', '8687293', '14319160', '19622226', '24822278', '34221240', '875292', '144165232', '198113108', '249115221', '3523564', '88257180', '145151168', '199284212', '25094225', '3697109', '8985202', '146108141', '200251215', '251118296', '37112267', '9044102', '147203295', '20145130', '252205199', '38995', '9178162', '1486646']\n",
            "182 files will be used for training\n",
            "39 files will be used for validating\n",
            "38 files will be used for testing\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'current_file' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-2452423926.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;32mglobal\u001b[0m \u001b[0mstereo_validation_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;32mglobal\u001b[0m \u001b[0mstereo_test_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m \u001b[0mmono_training_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mono\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0mmono_validation_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mono\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0mmono_test_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mono\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-2452423926.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filenames, batch_size, shuffle, model_type)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;31m# Update this path to your dataset directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/path/to/your/dataset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mdata_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_labels\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mopen_data_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_per_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_sizes\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Amount of batches in each file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-2452423926.py\u001b[0m in \u001b[0;36mopen_data_images\u001b[0;34m(file_path, model_type)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;31m#current_file = tables.open_file(file_path,'r')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# Masking out electron events and reading peak timing data with corresponding labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mmask_electrons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mpeak_times\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeak_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_electrons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mevent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_electrons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'current_file' is not defined"
          ]
        }
      ],
      "source": [
        "# Author: Saurabh Gangwar\n",
        "# Deep Learning Model to classify CTA events\n",
        "\n",
        "# In[ ]:\n",
        "#import modules\n",
        "\n",
        "import tables\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential, Input, Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Convolution2D, BatchNormalization, Dropout, TimeDistributed,LSTM\n",
        "from tensorflow.keras.layers import ReLU, MaxPool2D, AvgPool2D, GlobalAvgPool2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import keras_tuner as kt\n",
        "import time\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# Filenames of Data used for training, validation and testing.\n",
        "# No duplicates, so testing and validation are independant of training\n",
        "train_split, val_split, test_split = 0.7,0.15,0.15\n",
        "\n",
        "all_filenames = [1028155,    15760266,   21217180,   26227461, 47142250,\n",
        "10925472,   158272236,  2126965,    263287239,  48226249,\n",
        "110182113,  15929268,   21317083,   26411052,   4920265,\n",
        "111177194,  160200191,  214847,     26524833,   50197128,\n",
        "11252272,   162187274,  216259223,  267199235,  51224255,\n",
        "113264112,  16325284,   217167200,  268150297,  512938,\n",
        "114217205,  164277231,  21822021,   26953213,   527069,\n",
        "115238172,  1649133,    2183160,    270260186,  5518114,\n",
        "11680281,   165155260,  219207204,  2711201,    56243224,\n",
        "1172193,    16636157,   2207142,    2715567,    5721995,\n",
        "1175462,    16781288,   22168182,   272131233,  58276161,\n",
        "118164251,  1689582,    2224370,    273288152,  5919277,\n",
        "1194101,    169225189,  2229897,    27529287,   6092270,\n",
        "1198299,    170293184,  2233824,    276186179,  61190258,\n",
        "120111131,  17112323,   22410294,   27783174,   6144259,\n",
        "12124183,   17219311,   2258675,    27884169,   6226554,\n",
        "12189283,   17388146,   2265620,    27996195,   63262134,\n",
        "122236173,  17424071,   22739228,   280247206,  64297273,\n",
        "123252167,  175230129,  22813486,   281283175,  659188,\n",
        "125241227,  178148246,  2297798,    282273243,  6623344,\n",
        "126116211,  179253150,  23012589,   283138105,  67159298,\n",
        "12731132,   180146263,  231188190,  284166285,  6816,\n",
        "128179229,  181174149,  2329241,    2851192,    6929026,\n",
        "129175280,  182208135,  233015,     285189279,  7027085,\n",
        "130114118,  183169144,  233215156,  286117106,  71156247,\n",
        "131172185,  183737,     234239257,  28879125,   7217271,\n",
        "13227884,   18414366,   2354831,    28932196,   7310991,\n",
        "13275120,   185296220,  237295147,  29076110,   7465278,\n",
        "1332237,    186184207,  23862140,   2911539,    7515851,\n",
        "13421474,   187250165,  2394100,    293263264,  761520,\n",
        "135280254,  18864103,   2406394,    2942203,    77350,\n",
        "136122230,  189242,     24128257,   294244291,  7810587,\n",
        "137162261,  19020648,   242237158,  29721166,   7924276,\n",
        "138154218,  19107245,   2423275,    298267139,  8093176,\n",
        "139176124,  191140177,  2431556,    299286214,  8212712,\n",
        "14021241,   192232122,  24457143,   30271138,   8268111,\n",
        "141168290,  193294238,  24590248,   323173,     8312234,\n",
        "14119219,   19425536,   246120151,  323440,     851976,\n",
        "14240153,   1952918,    247289268,  33139299,   8687293,\n",
        "14319160,   19622226,   24822278,   34221240,   875292,\n",
        "144165232,  198113108,  249115221,  3523564,    88257180,\n",
        "145151168,  199284212,  25094225,   3697109,    8985202,\n",
        "146108141,  200251215,  251118296,  37112267,   9044102,\n",
        "147203295,  20145130,   252205199,  38995,      9178162,\n",
        "1486646,    203195171,  2526222,    39126115,   9227178,\n",
        "14913244,   20413093,   25524679,   40204289,   94285256,\n",
        "150133253,  20558117,   256201269,  416282,     96173210,\n",
        "15171159,   206132181,  257147137,  42137170,   9716334,\n",
        "1526190,    20715796,   258216262,  4279242,    974126,\n",
        "1537527,    2085035,    2594722,    43229198,   9833127,\n",
        "154213123,  20935163,   26016181,   44121277,   99196136,\n",
        "1545216,    21025892,   261227119,  45281252]\n",
        "\n",
        "filenames_train = []\n",
        "filenames_val = []\n",
        "filenames_test = []\n",
        "for i in range(len(all_filenames)):\n",
        "    if i/len(all_filenames) < train_split:\n",
        "        filenames_train.append(str(all_filenames[i]))\n",
        "    elif i/len(all_filenames) > 1 - test_split:\n",
        "        filenames_test.append(str(all_filenames[i]))\n",
        "    elif i/len(all_filenames) > 1 - val_split - test_split:\n",
        "        filenames_val.append(str(all_filenames[i]))\n",
        "\n",
        "print(filenames_test)\n",
        "print(filenames_val)\n",
        "print(len(filenames_train), \"files will be used for training\")\n",
        "print(len(filenames_val), \"files will be used for validating\")\n",
        "print(len(filenames_test), \"files will be used for testing\")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# Load data from files and for monoscopic data: clean\n",
        "def open_data_images(file_path, model_type):\n",
        "    # Loading data\n",
        "    current_file = tables.open_file(file_path,'r')\n",
        "    # Masking out electron events and reading peak timing data with corresponding labels\n",
        "    mask_electrons = current_file.root.event_label[:] != 2\n",
        "    peak_times = current_file.root.peak_times[:,:,:,:][mask_electrons]\n",
        "    event_labels = current_file.root.event_label[:][mask_electrons]\n",
        "    # Add extra dimension for channels\n",
        "    peak_times = peak_times.reshape((*np.shape(peak_times),1))\n",
        "    event_labels = event_labels.reshape((np.shape(event_labels)[0],1))\n",
        "\n",
        "    if model_type == \"mono\":\n",
        "        data_images = np.empty((0, 48,48,1))\n",
        "        data_labels = np.empty((0, 1))\n",
        "        # Add images from all 4 telescopes to one array\n",
        "        for i in range(0,4):\n",
        "            data_images = np.concatenate((data_images,peak_times[:,i,:,:]), axis = 0)\n",
        "            data_labels = np.concatenate((data_labels,event_labels[:]))\n",
        "        # Mask out images where all data is 0 for monoscopic training\n",
        "        zero_mask = np.sum(data_images, axis = 1) # sums over axis = 1,so from shape (N,48,48,1) to (N,48,1) (along one axis of the image)\n",
        "        zero_mask = np.sum(zero_mask, axis = 1) # sums over axis = 1,so from shape (N,48,1) to (N,1) (along other axis of the image)\n",
        "        zero_mask = np.squeeze(zero_mask) # goes from shape (N,1) to shape (N,)\n",
        "        zero_mask = zero_mask > 100 # True/False where sum of whole image is  > 0\n",
        "        data_images = data_images[zero_mask,:,:,:]\n",
        "        data_labels = data_labels[zero_mask]\n",
        "\n",
        "    if model_type == \"stereo\":\n",
        "        data_images = np.empty((0, 4, 48,48,1))\n",
        "        data_labels = np.empty((0, 1))\n",
        "        data_labels = np.concatenate((data_labels,event_labels[:]))\n",
        "        data_images = np.concatenate((data_images,peak_times), axis = 0)\n",
        "\n",
        "    return data_images, data_labels\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# loads batches of data\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, filenames, batch_size=64, shuffle=True, model_type = \"mono\"):\n",
        "        # Initialization\n",
        "        self.batch_size = batch_size # Batch Size\n",
        "        self.shuffle = shuffle # Shuffle the data files read and the batches within the files\n",
        "        self.model_type = model_type # stereo or mono\n",
        "        self.filenames = np.array(filenames) # Names of data files\n",
        "        self.file_sizes = np.array([]) # Amount of useable sets of images (if mono -> filtered) per file\n",
        "        self.next_file_no = 0 # Counter of which file is read next\n",
        "        self.current_batch = 0 # Counter of which is the current loaded batch\n",
        "\n",
        "        # Loop over all files to know how many batches are in each file (and therefore how many batches in total)\n",
        "        for filename in self.filenames:\n",
        "            # Update this path to your dataset directory\n",
        "            file_path = os.path.join(r'/path/to/your/dataset', filename + '.hdf5')\n",
        "            data_images, data_labels  = open_data_images(file_path, self.model_type)\n",
        "            self.file_sizes = np.append(self.file_sizes, np.shape(data_images)[0])\n",
        "            self.batches_per_file = np.floor(self.file_sizes/self.batch_size) # Amount of batches in each file\n",
        "\n",
        "        self.reset_counters() # Resets batch and file counters and shuffles them\n",
        "\n",
        "    def __len__(self): # Defines the Function len(DG), where DG is an object of class DataGenerator.\n",
        "        #len(DG) is called by model.fit and keras tuner to know how many batches are processed (example: the 1/527 counter)\n",
        "        return int(np.floor(np.sum(self.batches_per_file)))\n",
        "\n",
        "    def __getitem__(self, index): # Defines the Function DG[i], where DG is an object of class DataGenerator and i is an index.\n",
        "        #DG[i] is called by model.fit and keras tuner to open batch i/len(DG). In this function, the index i will\n",
        "        # be discarded. Every time an element is accessed via DG[x] the next batch of a file is loaded (maybe shuffled).\n",
        "        # If it would be batch 0, the file is loaded, and if it would be the last batch, the batch counter is reset to 0\n",
        "        if self.current_batch == 0: #Check if the current batch to load is the first batch\n",
        "\n",
        "            self.current_batch_mixed = np.arange(0, self.batches_per_file[self.next_file_no]+1) # Define order\n",
        "                # in which to open the batches of the file\n",
        "            if self.shuffle == True: # permutate the order in which the batches are opened in the file\n",
        "                p = np.random.permutation(len(self.current_batch_mixed)) # create permuation\n",
        "                self.current_batch_mixed = self.current_batch_mixed[p] #apply permutation\n",
        "            filename = self.filenames[self.next_file_no] # Get filename to open\n",
        "            self.next_file_no += 1 # Prepare next filename to open\n",
        "            # Update this path to your dataset directory\n",
        "            file_path = os.path.join(r'/path/to/your/dataset', filename + '.hdf5')\n",
        "            self.data_images, self.data_labels = open_data_images(file_path, self.model_type)\n",
        "\n",
        "        # Input (image) Data X of shape (batchsize, *dims)\n",
        "        # self.current_batch_mixed[self.current_batch] calls correct (shuffled) batch\n",
        "        # Example first batch (self.current_batch=0) which happens to shuffled to third position self.current_batch_mixed[0]=3\n",
        "        if self.model_type == \"mono\":\n",
        "            X = self.data_images[int(self.current_batch_mixed[self.current_batch]*self.batch_size):\n",
        "                                          int((self.current_batch_mixed[self.current_batch]+1)*self.batch_size),:,:,:]\n",
        "        if self.model_type == \"stereo\":\n",
        "            X = self.data_images[int(self.current_batch_mixed[self.current_batch]*self.batch_size):\n",
        "                                          int((self.current_batch_mixed[self.current_batch]+1)*self.batch_size),:,:,:,:]\n",
        "\n",
        "        # Output (label) data y of shape (batchsize, 1)\n",
        "        y = self.data_labels[int(self.current_batch_mixed[self.current_batch]*self.batch_size):\n",
        "                                      int((self.current_batch_mixed[self.current_batch]+1)*self.batch_size)]\n",
        "         # Convert y to shape (batchsize, 2), binary data\n",
        "        y = keras.utils.to_categorical(y, num_classes=2)\n",
        "\n",
        "        self.current_batch +=1 # End of batch loading, increase counter\n",
        "        # If it was last batch of file, reset batch counter to 0 so new file is loaded\n",
        "        if self.current_batch > self.batches_per_file[self.next_file_no-1]:\n",
        "            self.current_batch = 0\n",
        "        return X, y\n",
        "\n",
        "    def reset_counters(self): #Reset Batch and file counter\n",
        "        self.next_file_no = 0\n",
        "        self.current_batch = 0\n",
        "        # Shuffle the files\n",
        "        if self.shuffle == True:\n",
        "            p = np.random.permutation(len(self.filenames)) # create permuation\n",
        "            self.filenames = self.filenames[p]  # apply permutation to file names\n",
        "            self.file_sizes = self.file_sizes[p] # apply same permutation to file sizes\n",
        "            self.batches_per_file = self.batches_per_file[p] # apply same permutation to batches per file\n",
        "\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "# Method called at the beginning of each epoch to reset the batch counters of the data generators\n",
        "class OnEpochBegin(keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        mono_training_generator.reset_counters()\n",
        "        mono_validation_generator.reset_counters()\n",
        "        stereo_training_generator.reset_counters()\n",
        "        stereo_validation_generator.reset_counters()\n",
        "        print(\"Epoch Begin\")\n",
        "\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# Define Function to build and optimize a net for monoscopic image analysis\n",
        "def build_mono_net(hp):\n",
        "    inputs = Input(shape = (48, 48, 1)) #input layer\n",
        "    net = inputs\n",
        "\n",
        "    # Optimize amount of convolutional blocks\n",
        "    for i in range(hp.Int('conv_blocks',min_value = 3, max_value = 5, default=3)):\n",
        "        # Optimize amount of neurons per convolutional blocks\n",
        "        filters = hp.Int('filters_' + str(i), min_value = 32,max_value = 256, step=8)\n",
        "        conv_dropout_rate = hp.Float('conv_droput_rate_'+ str(i), min_value = 0.05, max_value = 0.5, step=0.05)\n",
        "\n",
        "        # Define the Dropout, conv, BatchNorm and activation layers for each block\n",
        "        net = Dropout(conv_dropout_rate)(net)\n",
        "        net = Convolution2D(filters, kernel_size=(3, 3), padding= 'same')(net)\n",
        "        #net = BatchNormalization()(net)\n",
        "        net = ReLU()(net)\n",
        "\n",
        "    # Optimize for MaxPooling or Average Pooling\n",
        "        if hp.Choice('pooling_' + str(i), ['avg', 'max']) == 'max':\n",
        "            net = MaxPool2D()(net)\n",
        "        else:\n",
        "            net = AvgPool2D()(net)\n",
        "\n",
        "    # Core net as global, so it can be saved after training\n",
        "    global core_net\n",
        "    core_net = Model(inputs,net)\n",
        "\n",
        "    net = Flatten()(net)\n",
        "\n",
        "     # Optimize amount of dense layers\n",
        "    for i in range(hp.Int('dense_layers',min_value = 1, max_value = 3, default=1)):\n",
        "        dense_dropout_rate = hp.Float('dense_droput_rate_'+ str(i), min_value = 0.05, max_value = 0.5, step=0.05)\n",
        "        dense_neurons = hp.Int('dense_neurons_' + str(i), min_value = 10, max_value = 200, step = 10, default = 50)\n",
        "        net = Dropout(dense_dropout_rate)(net)\n",
        "        net = Dense(dense_neurons, activation = \"relu\")(net)\n",
        "\n",
        "    # Output Layer\n",
        "    dense_dropout_rate = hp.Float('dense_droput_rate_-1', min_value = 0.05, max_value = 0.5, step=0.05)\n",
        "    net = Dropout(dense_dropout_rate)(net)\n",
        "    outputs = Dense(2, activation= 'sigmoid')(net)\n",
        "\n",
        "    # Create Model\n",
        "    learning_rate = hp.Float('learning_rate', min_value = 1e-4, max_value =1e-1, sampling='log')\n",
        "    mono_model = Model(inputs, outputs)\n",
        "    mono_model.compile(optimizer= Adam(learning_rate),\n",
        "                loss= 'BinaryCrossentropy', metrics = ['accuracy'])\n",
        "    return mono_model\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "core_net = None # Create Variable\n",
        "#initiate generators (as global, so their counters can be reset)\n",
        "global mono_training_generator\n",
        "global mono_validation_generator\n",
        "global mono_test_generator\n",
        "global stereo_training_generator\n",
        "global stereo_validation_generator\n",
        "global stereo_test_generator\n",
        "mono_training_generator = DataGenerator(filenames_train, batch_size=64, shuffle=True, model_type = \"mono\")\n",
        "mono_validation_generator = DataGenerator(filenames_val, batch_size=64, shuffle=True, model_type = \"mono\")\n",
        "mono_test_generator = DataGenerator(filenames_test, batch_size=64, shuffle=False, model_type = \"mono\")\n",
        "stereo_training_generator = DataGenerator(filenames_train, batch_size=64, shuffle=True, model_type = \"stereo\")\n",
        "stereo_validation_generator = DataGenerator(filenames_val, batch_size=64, shuffle=True, model_type = \"stereo\")\n",
        "stereo_test_generator = DataGenerator(filenames_test, batch_size=64, shuffle=False, model_type = \"stereo\")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# Initialize tuner to run the mono_model using the Hyperband search algorithm\n",
        "tuner = kt.Hyperband(\n",
        "    hypermodel = build_mono_net,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=30,\n",
        "    hyperband_iterations=2,\n",
        "    directory=\"MoDAII_Optimization\",\n",
        "    project_name=\"Mono_Model\")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(patience=3),\n",
        "             OnEpochBegin()]\n",
        "\n",
        "tuner.search(mono_training_generator,\n",
        "             validation_data= mono_validation_generator,\n",
        "             epochs=30,\n",
        "             callbacks=callbacks)\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_mono_hps= tuner.get_best_hyperparameters(1)[0]\n",
        "\n",
        "# Get the best model\n",
        "best_mono_model = tuner.get_best_models(1)[0]\n",
        "\n",
        "# Show model summary\n",
        "best_mono_model.summary()\n",
        "\n",
        "with open(r\"/home/hpc/b129dc/b129dc22/LargeDataSet/LargeDataSet_Summary_Monomodel.txt\", 'w') as f:\n",
        "    best_mono_model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# Integrate early stopping\n",
        "early_stopping =EarlyStopping(monitor='val_loss', patience=20)\n",
        "bst_mono_net_path = r'/home/hpc/b129dc/b129dc22/LargeDataSet/bst_mono_net_path.h5'\n",
        "mono_net_checkpoint = ModelCheckpoint(bst_mono_net_path, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "# Run Learning\n",
        "mono_net = tuner.hypermodel.build(best_mono_hps)\n",
        "history = mono_net.fit(mono_training_generator,\n",
        "          validation_data=mono_validation_generator,\n",
        "          epochs= 100,\n",
        "         callbacks=[early_stopping, mono_net_checkpoint, OnEpochBegin()])\n",
        "\n",
        "np.save(r'/home/hpc/b129dc/b129dc22/LargeDataSet/history_mono.npy',history.history)\n",
        "\n",
        "# Get best epoch and save mono and core nets (set all params non trainable, to avoid errors)\n",
        "for l in mono_net.layers:\n",
        "    l.trainable = False\n",
        "mono_net.load_weights(bst_mono_net_path)\n",
        "mono_net.save(\"mono_net\")\n",
        "core_net.save('core_net')\n",
        "\n",
        "# Evaluate Result\n",
        "eval_result = mono_net.evaluate(mono_test_generator)\n",
        "print(f\"test loss: {eval_result[0]}, test accuracy: {eval_result[1]}\")\n",
        "\n",
        "text_file = open(\"/home/hpc/b129dc/b129dc22/LargeDataSet/LargeDataSet_Testresults_Monomodel.txt\", \"w\")\n",
        "text_file.write(f\"test loss: {eval_result[0]}, test accuracy: {eval_result[1]}\")\n",
        "text_file.close()\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# Define Function to build and optimize a net for stereoscopic image analysis\n",
        "def build_stereo_net(hp):\n",
        "    core_net = keras.models.load_model('core_net') # Load core net from file\n",
        "    inputs = Input(shape = (4,48, 48, 1)) #input layer\n",
        "    net = inputs\n",
        "\n",
        "    net = TimeDistributed(core_net, trainable=False)(net) # Apply core net to all 4 telescoptes\n",
        "    net = TimeDistributed(Flatten())(net) # Necessary to input to LSTM\n",
        "\n",
        "    #Optimize amount of lstm blocks\n",
        "    for i in range(hp.Int('lstm_blocks',min_value = 0, max_value = 5, default=0)):\n",
        "        dropout_rate = hp.Float('lstm_droput_rate_'+str(i), min_value = 0.05, max_value = 0.5, step=0.05)\n",
        "        net = Dropout(dropout_rate)(net)\n",
        "        net = LSTM(4,return_sequences=True)(net)\n",
        "\n",
        "    dropout_rate = hp.Float('lstm_droput_rate_-1', min_value = 0.05, max_value = 0.5, step=0.05)\n",
        "    net = Dropout(dropout_rate)(net)\n",
        "    net = LSTM(4,return_sequences=False)(net)\n",
        "\n",
        "     # Optimize amount of dense layers\n",
        "    for i in range(hp.Int('dense_layers',min_value = 1, max_value = 5, default=2)):\n",
        "        dense_dropout_rate = hp.Float('dense_droput_rate_'+ str(i), min_value = 0.05, max_value = 0.5, step=0.05)\n",
        "        dense_neurons = hp.Int('dense_neurons_' + str(i), min_value = 10, max_value = 200, step = 10, default = 50)\n",
        "        net = Dropout(dense_dropout_rate)(net)\n",
        "        net = Dense(dense_neurons, activation = \"relu\")(net)\n",
        "\n",
        "    # Output Layer\n",
        "    dropout_rate = hp.Float('dense_droput_rate_-1', min_value = 0.05, max_value = 0.5, step=0.05)\n",
        "    net = Dropout(dropout_rate)(net)\n",
        "    outputs = Dense(2, activation= 'sigmoid')(net)\n",
        "\n",
        "    # Create Model\n",
        "    learning_rate = hp.Float('learning_rate', min_value = 1e-4, max_value =1e-1, sampling='log')\n",
        "    stereo_model = Model(inputs, outputs)\n",
        "    stereo_model.compile(optimizer= Adam(learning_rate),\n",
        "                loss= 'BinaryCrossentropy', metrics = ['accuracy'])\n",
        "    return stereo_model\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "# Initialize tuner to run the stereo_model using the Hyperband search algorithm\n",
        "tuner = kt.Hyperband(\n",
        "    hypermodel = build_stereo_net,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=30,\n",
        "    hyperband_iterations=2,\n",
        "    overwrite=False,\n",
        "    directory=\"MoDAII_Optimization\",\n",
        "    project_name=\"Stereo_Model\")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# Run the search for best stereo_model\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(patience=3),\n",
        "             OnEpochBegin()]\n",
        "\n",
        "tuner.search(stereo_training_generator,\n",
        "             validation_data= stereo_validation_generator,\n",
        "             epochs=30,\n",
        "             callbacks=callbacks)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_stereo_hps= tuner.get_best_hyperparameters(1)[0]\n",
        "\n",
        "# Get the best model\n",
        "best_stereo_model = tuner.get_best_models(1)[0]\n",
        "\n",
        "# Show model summary\n",
        "best_stereo_model.summary()\n",
        "\n",
        "with open(r\"/home/hpc/b129dc/b129dc22/LargeDataSet/LargeDataSet_Summary_Stereomodel.txt\", 'w') as f:\n",
        "    best_stereo_model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# Build the stereo_net with the optimal hyperparameters and train the model.\n",
        "# Integrate early stopping\n",
        "early_stopping =EarlyStopping(monitor='val_loss', patience=10)\n",
        "bst_stereo_net_path = r'/home/hpc/b129dc/b129dc22/LargeDataSet/bst_stereo_net_path.h5'\n",
        "stereo_net_checkpoint = ModelCheckpoint(bst_stereo_net_path, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "# Run Learning\n",
        "stereo_net = tuner.hypermodel.build(best_stereo_hps)\n",
        "history = stereo_net.fit(stereo_training_generator,\n",
        "          validation_data= stereo_validation_generator,\n",
        "          epochs= 100, batch_size = 64,\n",
        "         callbacks=[early_stopping, stereo_net_checkpoint, OnEpochBegin()])\n",
        "\n",
        "np.save(r'/home/hpc/b129dc/b129dc22/LargeDataSet/history_stereo.npy',history.history)\n",
        "\n",
        "# Get best epoch and save stereo net (set all params non trainable, to avoid errors)\n",
        "for l in stereo_net.layers:\n",
        "    l.trainable = False\n",
        "stereo_net.load_weights(bst_stereo_net_path)\n",
        "stereo_net.save(\"stereo_net\")\n",
        "\n",
        "# Evaluate the result\n",
        "eval_result = stereo_net.evaluate(stereo_test_generator)\n",
        "print(f\"test loss: {eval_result[0]}, test accuracy: {eval_result[1]}\")\n",
        "\n",
        "text_file = open(\"/home/hpc/b129dc/b129dc22/LargeDataSet/LargeDataSet_Testresults_Stereomodel.txt\", \"w\")\n",
        "text_file.write(f\"test loss: {eval_result[0]}, test accuracy: {eval_result[1]}\")\n",
        "text_file.close()\n",
        "\n",
        "# In[ ]:\n",
        "########################################### From here: Plotting\n",
        "\n",
        "#plotting the history of a net training\n",
        "def history_analysis(history, name=\"Test\"):\n",
        "    fig, ax = plt.subplots(1,2, figsize = (12,4))\n",
        "    #plot accuracy\n",
        "    #plt.rcParams.update({'font.size': fontssize})\n",
        "    ax[0].plot(history['accuracy'], label='Accuracy',color='red')\n",
        "    ax[0].plot(history['val_accuracy'], label = 'Validation accuracy',color='blue')\n",
        "    ax[0].set_xlabel('Epoch')\n",
        "    ax[0].set_ylabel('Accuracy')\n",
        "    ax[0].set_ylim([0.5, 1])\n",
        "    ax[0].legend(loc='best')\n",
        "    ax[0].grid(alpha=0.6)\n",
        "    #plot loss\n",
        "    ax[1].plot(history['loss'],color='red',ls='dashed')\n",
        "    ax[1].plot(history['val_loss'],color='blue',ls='dashed')\n",
        "    ax[1].set_ylabel('Loss')\n",
        "    ax[1].set_xlabel('Epoch')\n",
        "    ax[1].legend(['Loss', 'Validation Loss'], loc='best')\n",
        "    ax[1].grid(alpha=0.6)\n",
        "    file_path = os.path.join(r\"/home/hpc/b129dc/b129dc22/Analysis\", name + '_history.png')\n",
        "    plt.savefig(file_path)\n",
        "    plt.show()\n",
        "\n",
        "    #plot the ROC curve of a model\n",
        "def plot_roc_curve(net, X_truth, y_truth, name = \"Large Data Set\", batch_size = 64):\n",
        "    y_pred_keras = net.predict(X_truth, batch_size=batch_size)[:,1] #Column 0 is prob. of being 0 and Column 2 is prob of being 1\n",
        "    #eval_result = net.evaluate(test_generator)\n",
        "    #print(f\"test loss: {eval_result[0]}, test accuracy: {eval_result[1]}\")\n",
        "    fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_truth, y_pred_keras)\n",
        "    auc_keras = auc(fpr_keras, tpr_keras)\n",
        "    plt.plot(fpr_keras, tpr_keras, label=name +' (area = {:.3f})'.format(auc_keras))\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "# Plot history of Small Mono Model\n",
        "\n",
        "history=np.load(r\"/home/hpc/b129dc/b129dc22/SmallDataSet/history_mono.npy\",\n",
        "                allow_pickle='TRUE').item()\n",
        "#mono_test_prediction = mono_net.predict(mono_test_generator)\n",
        "history_analysis(history, name = \"mono_net_small\")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "# Plot history of Mid Mono Model\n",
        "\n",
        "history=np.load(r\"/home/hpc/b129dc/b129dc22/MidDataSet/history_mono.npy\",\n",
        "                allow_pickle='TRUE').item()\n",
        "#mono_test_prediction = mono_net.predict(mono_test_generator)\n",
        "history_analysis(history, name = \"mono_net_mid\")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "# Plot history of Large Mono Model\n",
        "\n",
        "history=np.load(r\"/home/hpc/b129dc/b129dc22/LargeDataSet/history_mono.npy\",\n",
        "                allow_pickle='TRUE').item()\n",
        "#mono_test_prediction = mono_net.predict(mono_test_generator)\n",
        "history_analysis(history,name = \"mono_net_large\")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "# Load X_truth and y_truth to variables\n",
        "\n",
        "mono_test_generator.reset_counters()\n",
        "y_truth = np.empty((0,2))\n",
        "X_truth = np.empty((0,48,48,1))\n",
        "for i in range(len(mono_test_generator)):\n",
        "    next_batch_X, next_batch_y = mono_test_generator[i]\n",
        "    y_truth = np.concatenate((y_truth, next_batch_y))\n",
        "    X_truth = np.concatenate((X_truth, next_batch_X))\n",
        "y_truth = y_truth[:,1]\n",
        "\n",
        "print(\"Loading Data into Variables DONE\")\n",
        "\n",
        "\n",
        "# Plot ROC curve of Mono Model\n",
        "\n",
        "plt.figure(figsize = (5,5), dpi = 600)\n",
        "mono_net = tf.keras.models.load_model(r\"/home/hpc/b129dc/b129dc22/SmallDataSet/mono_net\")\n",
        "plot_roc_curve(mono_net,  X_truth, y_truth,  name = \"Small Data Set\")\n",
        "print(\"ROC 1 DONE\")\n",
        "mono_net = tf.keras.models.load_model(r\"/home/hpc/b129dc/b129dc22/MidDataSet/mono_net\")\n",
        "plot_roc_curve(mono_net,  X_truth, y_truth,  name = \"Mid Data Set\")\n",
        "print(\"ROC 2 DONE\")\n",
        "mono_net = tf.keras.models.load_model(r\"/home/hpc/b129dc/b129dc22/LargeDataSet/mono_net\")\n",
        "plot_roc_curve(mono_net,  X_truth, y_truth,  name = \"Large Data Set\")\n",
        "print(\"ROC 3 DONE\")\n",
        "\n",
        "\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.grid(alpha=0.6)\n",
        "plt.xlim(0,1)\n",
        "plt.ylim(0,1)\n",
        "plt.legend()\n",
        "file_path = (r\"/home/hpc/b129dc/b129dc22/Analysis/mono_net_roc.png\")\n",
        "plt.savefig(file_path)\n",
        "plt.show()\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "# Plot history of Small Stereo Model\n",
        "\n",
        "history=np.load(r\"/home/hpc/b129dc/b129dc22/SmallDataSet/history_stereo.npy\",\n",
        "                allow_pickle='TRUE').item()\n",
        "#mono_test_prediction = mono_net.predict(mono_test_generator)\n",
        "\n",
        "history_analysis(history,name = \"stereo_net_small\")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "    # Plot history of Mid Stereo Model\n",
        "\n",
        "history=np.load(r\"/home/hpc/b129dc/b129dc22/MidDataSet/history_stereo.npy\",\n",
        "                allow_pickle='TRUE').item()\n",
        "#mono_test_prediction = mono_net.predict(mono_test_generator)\n",
        "\n",
        "history_analysis(history, name = \"stereo_net_mid\")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "# Plot history of Large Stereo Model\n",
        "\n",
        "history=np.load(r\"/home/hpc/b129dc/b129dc22/LargeDataSet/history_stereo.npy\",\n",
        "                allow_pickle='TRUE').item()\n",
        "#mono_test_prediction = mono_net.predict(mono_test_generator)\n",
        "history_analysis(history, name = \"stereo_net_large\")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "# Load X_truth and y_truth to variables\n",
        "\n",
        "stereo_test_generator.reset_counters()\n",
        "y_truth = np.empty((0,2))\n",
        "X_truth = np.empty((0,4,48,48,1))\n",
        "for i in range(len(stereo_test_generator)):\n",
        "    next_batch_X, next_batch_y = stereo_test_generator[i]\n",
        "    y_truth = np.concatenate((y_truth, next_batch_y))\n",
        "    X_truth = np.concatenate((X_truth, next_batch_X))\n",
        "y_truth = y_truth[:,1]\n",
        "print(\"Loading Data into Variables DONE\")\n",
        "\n",
        "# Plot ROC curve of Stereo Model\n",
        "\n",
        "plt.figure(figsize = (5,5), dpi = 600)\n",
        "stereo_net = tf.keras.models.load_model(r\"/home/hpc/b129dc/b129dc22/SmallDataSet/stereo_net\")\n",
        "plot_roc_curve(stereo_net, X_truth, y_truth, name = \"Small Data Set\")\n",
        "print(\"ROC 1 DONE\")\n",
        "stereo_net = tf.keras.models.load_model(r\"/home/hpc/b129dc/b129dc22/MidDataSet/stereo_net\")\n",
        "plot_roc_curve(stereo_net, X_truth, y_truth, name = \"Mid Data Set\")\n",
        "print(\"ROC 2 DONE\")\n",
        "stereo_net = tf.keras.models.load_model(r\"/home/hpc/b129dc/b129dc22/LargeDataSet/stereo_net\")\n",
        "plot_roc_curve(stereo_net, X_truth, y_truth, name = \"Large Data Set\")\n",
        "print(\"ROC 3 DONE\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.grid(alpha=0.6)\n",
        "plt.legend()\n",
        "plt.xlim(0,1)\n",
        "plt.ylim(0,1)\n",
        "file_path = (r\"/home/hpc/b129dc/b129dc22/Analysis/stereo_net_roc.png\")\n",
        "plt.savefig(file_path)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_tuner\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC9zwky8IobH",
        "outputId": "8d05b220-787a-4b31-f33a-d39c16f4990e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras_tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras_tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras_tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras_tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (0.16.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (2025.6.15)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras_tuner) (4.14.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras_tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras_tuner) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras_tuner\n",
            "Successfully installed keras_tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    }
  ]
}